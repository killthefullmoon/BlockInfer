#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
é€šç”¨ AI ç¯å¢ƒå®Œæ•´æµ‹è¯•è„šæœ¬ test_env.py
è‡ªåŠ¨æ£€æµ‹ Python / Torch / CUDA / FlashAttention / CXX11 ABI ç­‰ä¿¡æ¯
æœ€åè¾“å‡ºåŸºäºæ£€æµ‹ç»“æœçš„ç²¾ç‚¼æ€»ç»“
"""

import os
import sys
import subprocess
import platform
import traceback

summary = []  # æ”¶é›†æ€»ç»“ä¿¡æ¯


def ok(msg):
    summary.append("âœ” " + msg)
    print("âœ”", msg)


def warn(msg):
    summary.append("âš  " + msg)
    print("âš ", msg)


def fail(msg):
    summary.append("âŒ " + msg)
    print("âŒ", msg)


print("=" * 60)
print("ğŸ” Python ç¯å¢ƒä¿¡æ¯")
print("=" * 60)

print("Python executable:", sys.executable)
print("Python version:", sys.version.split()[0])
print("Platform:", platform.platform())
ok("Python è¿è¡Œæ­£å¸¸")

print("\n" + "=" * 60)
print("ğŸ” ç³»ç»Ÿ CUDA ä¸é©±åŠ¨æ£€æµ‹")
print("=" * 60)


def run_cmd(cmd):
    try:
        return subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT).decode()
    except Exception:
        return ""


nvidia_smi = run_cmd("nvidia-smi")
print("nvidia-smi:")
print(nvidia_smi)
if "Driver Version" in nvidia_smi:
    ok("æ£€æµ‹åˆ° NVIDIA GPU é©±åŠ¨")
else:
    warn("æœªæ£€æµ‹åˆ°æœ‰æ•ˆçš„ NVIDIA é©±åŠ¨")

nvcc = run_cmd("nvcc --version")
print("nvcc version:")
print(nvcc)
if "release" in nvcc:
    ok("æ£€æµ‹åˆ° nvcc ç¼–è¯‘å™¨")
else:
    warn("æœªæ£€æµ‹åˆ° nvccï¼ˆCUDA Toolkit å¯èƒ½æœªå®‰è£…ï¼‰")

print("CUDA_HOME:", os.environ.get("CUDA_HOME"))
print("LD_LIBRARY_PATH:", os.environ.get("LD_LIBRARY_PATH"))


print("\n" + "=" * 60)
print("ğŸ” PyTorch æµ‹è¯•")
print("=" * 60)

torch_available = False
cuda_available = False
abi_flag = None

try:
    import torch
    torch_available = True
    print("PyTorch version:", torch.__version__)
    print("PyTorch built CUDA:", torch.version.cuda)
    print("cuDNN:", torch.backends.cudnn.version())

    ok("PyTorch å¯¼å…¥æˆåŠŸ")

    if torch.cuda.is_available():
        cuda_available = True
        ok("PyTorch CUDA å¯ç”¨")
        print("GPU Name:", torch.cuda.get_device_name(0))
        print("GPU Capability:", torch.cuda.get_device_capability(0))
    else:
        warn("PyTorch æœªæ£€æµ‹åˆ° GPU")

    print("\nPyTorch Config:")
    config = torch.__config__.show()
    abi_flag = "1" if "TORCH_CXX11_ABI: 1" in config else "0"
    print(config)
    ok(f"TORCH_CXX11_ABI = {abi_flag}")

except Exception:
    fail("PyTorch æµ‹è¯•å¤±è´¥")
    traceback.print_exc()


print("\n" + "=" * 60)
print("ğŸ” FlashAttention æµ‹è¯•")
print("=" * 60)

flash_ok = False
flash_kernel_ok = False

try:
    import flash_attn
    flash_ok = True
    ok(f"FlashAttention å¯¼å…¥æˆåŠŸ: {flash_attn.__version__}")

    try:
        from flash_attn.flash_attn_interface import flash_attn_func
        ok("FlashAttention CUDA æ‰©å±•å·²åŠ è½½")

        # FlashAttention kernel test (correct shape)
        if cuda_available:
            try:
                B, S, H, D = 1, 64, 8, 64
                q = torch.randn(B, S, H, D, dtype=torch.float16, device="cuda")
                k = torch.randn(B, S, H, D, dtype=torch.float16, device="cuda")
                v = torch.randn(B, S, H, D, dtype=torch.float16, device="cuda")

                out = flash_attn_func(q, k, v)
                flash_kernel_ok = True
                ok("FlashAttention kernel æµ‹è¯•æˆåŠŸ")
            except Exception:
                fail("FlashAttention kernel æ‰§è¡Œå¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬/ABI/å½¢çŠ¶é—®é¢˜ï¼‰")
                traceback.print_exc()
        else:
            warn("CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ FlashAttention kernel æµ‹è¯•")

    except Exception:
        fail("FlashAttention CUDA æ‰©å±•åŠ è½½æˆ–è¿è¡Œå¤±è´¥")
        traceback.print_exc()

except Exception:
    fail("æ— æ³•å¯¼å…¥ FlashAttention")
    traceback.print_exc()


print("\n" + "=" * 60)
print("ğŸ” æ€»ç»“")
print("=" * 60)

# ç²¾ç‚¼ç»“è®ºé€»è¾‘
if not torch_available:
    print("âŒ PyTorch å¼‚å¸¸ï¼šæ— æ³•è¿è¡Œ AI ç›¸å…³ä»»åŠ¡")
elif torch_available and not cuda_available:
    print("âš  PyTorch å·²å®‰è£…ï¼Œä½†æœªæ£€æµ‹åˆ° GPUï¼ˆå¯èƒ½æ˜¯é©±åŠ¨æˆ– CUDA é…ç½®é—®é¢˜ï¼‰")
elif torch_available and cuda_available and abi_flag == "0":
    print("âš  GPU å¯ç”¨ï¼Œä½† PyTorch ä½¿ç”¨æ—§ ABIï¼ˆå¯èƒ½å¯¼è‡´æ‰©å±•åº“ä¸å…¼å®¹ï¼‰")
elif flash_ok and not flash_kernel_ok:
    print("âš  FlashAttention åŠ è½½æˆåŠŸä½† kernel å¤±è´¥ï¼ˆå¤šä¸º CUDA/PT ABI ç‰ˆæœ¬ä¸ä¸€è‡´ï¼‰")
elif flash_ok and flash_kernel_ok:
    print("âœ” ç³»ç»Ÿå·²æˆåŠŸåŠ è½½ PyTorch + CUDA + FlashAttentionï¼ˆç¯å¢ƒæ­£å¸¸ï¼‰")
else:
    print("âš  éƒ¨åˆ†æ¨¡å—å¯ç”¨ï¼Œä½†æœªå®Œå…¨é€šè¿‡æµ‹è¯•")

print("\nè¯¦ç»†çŠ¶æ€ï¼š")
for s in summary:
    print(s)
